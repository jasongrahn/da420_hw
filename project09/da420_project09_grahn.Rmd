---
title: "project09"
author: "jason grahn"
date: "3/8/2019"
output: github_document
---

```{r setup and package load, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
library(tidyverse) # because I like the tools we have here. 
library(caret)
library(class) # the class package is used for simple KNN modeling.
library(ggcorrplot) # to make pretty correlation plots
library(kableExtra) # to make pretty tables
library(moderndive) # sampling
```

Use TWO of the supervised learning predictive modeling methods (e.g., neural network, support vector machine, and na√Øve Bayes) you choose to do the prediction on the Iris Data Set. 

* Compare the performance (i.e., accuracy) of the two methods. 
* Please describe each method you used, 
* the R/SAS package you used, 
* the performance (i.e., accuracy) comparison of the two methods. 

Make sure you include the commands and outputs, as well as the interpretations of each output.

```{r}
#load data
data <- iris 

# i dont like Camel case headers, so I'll correct those too
data <- data.table::setnames(data, tolower(names(iris[1:5])))

#show head of the data
head(data,5) %>% 
  knitr::kable()
```

```{r summary stats}
summary(iris)
```

A short exploration of the data shows sepal width has the the smallest variation of means to it's median, while petal length appears to have the largest differences. We have the same number of species in the dataset which will make it ideal for sampling. 

# Visualization
```{r histogram and density plotting, message=FALSE, warning=FALSE}
# I like a common theme to my plots
common_theme <- theme_light() +
  theme(legend.position = "bottom")

# histograms for each 
histogram <- iris %>% 
  ggplot() +
  geom_histogram(aes(x = petal.length, fill=species)) +
  labs(title = "Histogram") +
  common_theme

# density plots for each
density <- iris %>% 
  ggplot() + 
  geom_density(aes(x=petal.length, fill=species), alpha = 0.5) +
  labs(title = "Density") +
  common_theme

#put these together into a pretty picture
cowplot::plot_grid(histogram, density)
```


The histogram shows setosa clearly sits on it's own with very short petal lengths; but virginica and veriscolor have at least some degree of overlap.

Density plotting reinforces the histogram. It does a better job showing the overlap between virginica and versicolor. 
```{r scatter plotting}
sepal.scatter <- iris %>% ggplot() + 
  geom_point(aes(x = sepal.length, y = sepal.width, color = species)) +
  labs(title = "Sepal vars") +
  common_theme

petal.scatter <- iris %>% ggplot() + 
  geom_point(aes(x = petal.length, y = petal.width, color = species)) +
  labs(title = "Petal vars") +
  common_theme

cowplot::plot_grid(sepal.scatter, petal.scatter)
```

Scatterplotting for sepal variables shows similar behaviors to what we saw in the histogram and density plots. Clustering of the setosa lengths and widths and mixtures of versicolor and viginica points. The petal variables are a bit more defined. Setosa certainly sits on it's own, and now we can see better definitions between versicolor and virginica species; with virginica having (generally) longer and wider petals than versicolor.

## Building sample vs training data

### Use two thirds of data of each class (i.e., Setosa, Versicolour and Virginica) as training set and do the prediction on the rest. 

The first step in making our train v test dataset would be to assign random values to each row, then pull those out. I dont know how to do that in one full swoop, so I'll subset each species, build some 1's and 2's, assign _those_ to each subset, put them all back together, then filter out train vs test with the 1's and 2's. I'm 100% positive there's a better way to do this, probably with one simple package; but this is what I _know_, so that's what I'll run with. We __should__ be able to use the train vs test data for both methods. 

```{r create the train vs test data separation}
set.seed(666)

setosa.sub <- data %>% filter(species == "setosa")
versicolor.sub <- data %>% filter(species == "versicolor") 
virginica.sub <- data %>% filter(species == "virginica") 

set.seed(666)

ind <- sample(2, 
              nrow(setosa.sub), 
              replace=TRUE,
              prob=c(0.67, 0.33))

#tibble(ind) %>% group_by(ind) %>% summarize(n = n())
setosa.sub <- setosa.sub %>% mutate(ind = ind) 
versicolor.sub <- versicolor.sub %>% mutate(ind = ind) 
virginica.sub <- virginica.sub %>% mutate(ind = ind)

data.joined <- setosa.sub %>% 
  full_join(versicolor.sub) %>% 
  full_join(virginica.sub)

# make training set and labels
data.training <- data.joined %>% 
  filter(ind == 1) %>% 
  dplyr::select(sepal.length, sepal.width, petal.length, petal.width)

data.trainLabels <- data.joined[ind==1,5]
# make testing set and labels
data.test <- data.joined %>% 
  filter(ind == 2) %>% 
  dplyr::select(sepal.length, sepal.width, petal.length, petal.width)

data.testLabels <- data.joined[ind==2, 5]
```

# K-Nearest Neighbors 

$K$-Nearest Neighbors uses the neighbors of a given datapoint in order to classify a new datapoint. It's called $k$-nearest because the value $k$ determines how many neighbor points are used in the classification. If too little neighbors are used, we may over-fit the data; while using too many neighbors overly "smooths" out the data and leads to more generalization, resulting in less classifications.

To execute KNN, we can use the `class` package. It's good to reinforce the education we've received so we use this this package because it requires minimal inputs and we've been trained on it already. `Class` contains the `knn` function which requires, quite simply, your training dataset, your testing dataset, what your selected _k-value_ is, and how to label the output. 

```{r}
data.knn <- knn(train = data.training,
                test = data.test, 
                k = 5,
                cl = data.trainLabels)
table(data.knn)
```

```{r accuracy table}
table(data.testLabels, data.knn)

accuracy.table <- data.joined %>% 
  filter(ind == 2) %>% 
  mutate(data.knn = data.knn,
         predict.true = if_else(species == data.knn, TRUE, FALSE)) 

accuracy.rate <- accuracy.table %>% 
  group_by(predict.true) %>% 
  summarize(count = n(),
            rate = count / nrow(accuracy.table) * 100,
            rate = round(rate,2))

accuracy.rate %>% 
  knitr::kable()
```

Nice! We have `r accuracy.rate[2,3]` percent accuracy!